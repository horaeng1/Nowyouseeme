# Audio Description Evaluation Configuration
# ============================================
#
# Usage:
#   python -m eval_metric --config default_config.yaml
#
# You can override any setting via command-line arguments:
#   python -m eval_metric --config config.yaml --matcher dp --metric bertscore

# Input files (can be overridden via CLI)
generated_file: null  # Path to generated AD JSON file
reference_file: null  # Path to reference AD CSV file

# Matcher configuration
# Available methods: cluster, dp, overlap
matcher:
  method: cluster         # cluster: N:M matching, dp: 1:1 with gaps, overlap: 1:N
  min_overlap_sec: 0.5   # Minimum time overlap in seconds to consider a match
  
  # DP-specific parameters (only used when method: dp)
  w_time: 0.3            # Weight for time similarity (0-1)
  w_text: 0.7            # Weight for text similarity (0-1)
  gap_penalty_gen: -0.2  # Penalty for skipping generated items
  gap_penalty_ref: -0.2  # Penalty for skipping reference items
  time_scale: 10.0       # Scale for soft time similarity
  time_soft: true        # Use soft time similarity (vs temporal IoU)

# LLM Evaluation (Gemini)
llm:
  api_key: null          # Uses GEMINI_API_KEY env var if null
  model: gemini-2.5-flash
  max_retries: 3
  retry_delay: 2.0

# BERTScore
bertscore:
  model: roberta-large   # Options: roberta-large, bert-base-uncased, etc.
  device: null           # auto-detect if null, or specify 'cuda:0' / 'cpu'
  batch_size: 64
  rescale_with_baseline: false

# METEOR
meteor:
  alpha: 0.9             # Precision weight
  beta: 3.0              # Recall weight
  gamma: 0.5             # Fragmentation penalty

# CIDEr
cider:
  n_gram: 4              # Maximum n-gram size (1-4)
  sigma: 6.0             # Length penalty sigma

# CRITIC (Character Identification)
critic:
  characters: []         # List of character names, e.g. ["Tim", "Mary", "Dad"]
  characters_file: null  # Or path to JSON with character list
  device: cpu            # 'cuda:0' for GPU

# Output configuration
output:
  output_dir: null       # Uses input file directory if null
  save_detailed_csv: true
  save_summary_json: true
  include_timestamp: true

# Evaluators to run (empty = must specify via CLI)
# Options: llm, bertscore, meteor, cider, critic
evaluators: []

