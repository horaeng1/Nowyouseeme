"""
Jack AD Generation Module
=========================
High-quality Audio Description generation using Gemini 3 Pro Preview.

Pipeline:
1. Metadata extraction (scene analysis)
2. Core AD generation (silent segment detection + description)
3. STT extraction (dialogue transcription)
4. Final Integration (combine all three)
5. Stage2 compression (TTS duration-based)

Based on: jack_ad_en.py, jack_ad_ko.py (2024-12)
Model: Gemini 3 Pro Preview (gemini-3-pro-preview)
"""

import logging
import os
import json
import re
import asyncio
import time
import argparse
from typing import List, Dict, Any, Tuple
from concurrent.futures import ThreadPoolExecutor

try:
    from google import genai
    from google.genai import types
    from google.genai.errors import ServerError
except ImportError as e:
    try:
        import google.generativeai as genai
        import google.generativeai.types as types
        ServerError = Exception  # Fallback
    except ImportError:
        raise ImportError(
            "google-genai package is not installed. "
            "Please install it with: pip install google-genai"
        ) from e


logger = logging.getLogger(__name__)

# ==============================================================================
# Configuration
# ==============================================================================
GEMINI_MODEL_VISION = "gemini-3-pro-preview"
GEMINI_MODEL_TEXT = "gemini-3-pro-preview"

# Thread pool for async operations
executor = ThreadPoolExecutor(max_workers=4)

# ==============================================================================
# Prompts - Korean (synced with jack_ad_ko.py)
# ==============================================================================
PROMPT_METADATA_KO = """
ë‹¹ì‹ ì€ ì˜ìƒ ìŠ¤í† ë¦¬ ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ëª©í‘œ:
ì˜ìƒì—ì„œ ì‹œê°ì ìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥í•œ í•µì‹¬ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬,
ê³ í’ˆì§ˆ Audio Description ì œì‘ ì°¸ê³ ìš© JSONìœ¼ë¡œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.

ê·œì¹™:
- ë“±ì¥ì¸ë¬¼ ì´ë¦„ì€ í™”ë©´ì—ì„œ ì‹ë³„ ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ ê¸°ì….
- ì´ë¦„ì„ ì•Œ ìˆ˜ ì—†ìœ¼ë©´ "í™”ì" ë“± ì¼ë°˜ ëª…ì¹­ ì‚¬ìš©.
- ì œëª©ì€ ì œê³µëœ ê²½ìš°ë§Œ ì°¸ê³ , ì¶”ì¸¡ ê¸ˆì§€.
- ì˜ìƒì—ì„œ í™•ì¸ ë¶ˆê°€í•œ ì •ë³´ ì ˆëŒ€ ìƒì„± ê¸ˆì§€.
- ê°ì •Â·ë‚´ë©´ ë¬˜ì‚¬ëŠ” ì‹œê°ì  ê·¼ê±°ê°€ ëª…í™•í•  ë•Œë§Œ ì œí•œì ìœ¼ë¡œ í—ˆìš©.
- JSON ì™¸ í…ìŠ¤íŠ¸, ì£¼ì„, ì„¤ëª… ì¶œë ¥ ê¸ˆì§€.

ì¶œë ¥ í˜•ì‹(ë³€ê²½ ë¶ˆê°€):
{
  "video_title": "ì œê³µëœ ê²½ìš°ì—ë§Œ ê¸°ì…, ì—†ìœ¼ë©´ null",
  "overall_summary": "ì˜ìƒ ì „ì²´ íë¦„ì„ ì‹œê° ê¸°ë°˜ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì„¤ëª…",
  "scenes": [
    {
      "scene_id": "Scene-1",
      "start_time": "0:00.0",
      "end_time": "0:05.8",
      "summary": "ìŠ¤í† ë¦¬ ì´í•´ì— í•„ìš”í•œ ì‹œê° ì •ë³´ ìš”ì•½",
      "characters": [
        {
          "id": "char_1",
          "name": "í™”ë©´ì—ì„œ ì‹ë³„ëœ ì´ë¦„(ë¶ˆê°€ ì‹œ null)",
          "appearance": "ì™¸í˜• ì •ë³´",
          "visible_emotion": "í‘œì • ê¸°ë°˜ ê°ì •(ëª…í™• ì‹œë§Œ)"
        }
      ],
      "visible_actions": [
        "í™•ì‹¤íˆ ë³´ì´ëŠ” ì£¼ìš” í–‰ë™"
      ],
      "relationships": [
        "ì‹œê° ê·¼ê±° ìˆëŠ” ê´€ê³„ë§Œ"
      ],
      "visual_focus": "ì¥ë©´ì—ì„œ ì‹œê°ì  ì¤‘ì‹¬ ìš”ì†Œ"
    }
  ]
}
"""

PROMPT_AD_KO = """
ë‹¹ì‹ ì˜ ìµœìš°ì„  ì„ë¬´ëŠ” ì˜ìƒì˜ ì˜¤ë””ì˜¤ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ,
ëŒ€ì‚¬ê°€ í¬í•¨ë˜ì§€ ì•Šì€ ëª¨ë“  ë¬´ìŒ êµ¬ê°„ì„ ì •í™•íˆ íƒì§€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

[ê·œì¹™]

ë¬´ìŒ êµ¬ê°„ íƒì§€
1) ì˜¤ë””ì˜¤ ì‹ í˜¸ ë¶„ì„ì„ ê¸°ì¤€ìœ¼ë¡œ ëŒ€ì‚¬Â·ë‚´ë ˆì´ì…˜Â·ë§ì†Œë¦¬ í¬í•¨ êµ¬ê°„ì€ ì œì™¸
2) 2.5ì´ˆ ì´ìƒ ì§€ì†ëœ ë¬´ìŒ êµ¬ê°„ì€ ë°˜ë“œì‹œ ëª¨ë‘ íƒì§€í•˜ì—¬ JSON ë°°ì—´ì— í¬í•¨
3) 2.5ì´ˆ ë¯¸ë§Œ êµ¬ê°„ì€ ì¸ì ‘ êµ¬ê°„ê³¼ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ë  ê²½ìš°ì—ë§Œ ë³‘í•© ê°€ëŠ¥
4) ë¬´ìŒ êµ¬ê°„ì„ í•˜ë‚˜ë§Œ ì„ íƒí•˜ê±°ë‚˜ ì„ì˜ë¡œ ì¶•ì†Œ ë˜ëŠ” ìƒëµ ê¸ˆì§€
5) ë¬´ìŒ êµ¬ê°„ì˜ ì‹œì‘/ë ì‹œê°ì„ ì„ì˜ë¡œ ì¡°ì •í•˜ê±°ë‚˜ ìƒì„± ê¸ˆì§€

íƒ€ì„ìŠ¤íƒ¬í”„
6) start_time, end_timeì„ ì‹¤ì œ ë¬´ìŒ ì‹œì‘ê³¼ ëìœ¼ë¡œ ì •í™•í•˜ê²Œ ê¸°ì…
7) duration_sec = end_time - start_timeì„ ë°˜ë“œì‹œ ì •í™•í•˜ê²Œ ê³„ì‚°í•˜ì—¬ ê¸°ì…
8) íƒ€ì„ìŠ¤íƒ¬í”„ì˜ ìˆœì„œê°€ ì‹¤ì œ ì˜ìƒ ì§„í–‰ê³¼ ë™ì¼í•´ì•¼ í•˜ë©°, ê²¹ì¹˜ê±°ë‚˜ ëˆ„ë½ëœ ì‹œê°„ ì¡´ì¬ ê¸ˆì§€

ìŒì„±í•´ì„¤(Description) ì‘ì„±
9) ë¬´ìŒ êµ¬ê°„ ë™ì•ˆ í™”ë©´ì—ì„œ ì§ì ‘ í™•ì¸ ê°€ëŠ¥í•œ í•µì‹¬ ì‹œê°ì •ë³´ë¥¼ ìƒì„¸í•˜ê²Œ ë¬˜ì‚¬
10) ë©”íƒ€ë°ì´í„°(ë“±ì¥ì¸ë¬¼, ë°°ê²½, ìƒí™©)ëŠ” í™”ë©´ìœ¼ë¡œ ê²€ì¦ë˜ëŠ” ê²½ìš°ì—ë§Œ í™œìš©
11) ì¥ë©´ ë‚´ ë³€í™”ê°€ ìˆìœ¼ë©´ í•œ ë¬¸ì¥ì— ì••ì¶•í•˜ì§€ ë§ê³  í•µì‹¬ ìš”ì†Œ ìš°ì„  ë°°ì¹˜
12) ê°ì •Â·ë‚´ë©´ ë¬˜ì‚¬ ê¸ˆì§€ (í‘œì •, í–‰ë™ ë“± ì‹œê°ì  ê·¼ê±°ê°€ ìˆì„ ê²½ìš°ë§Œ ì œí•œì ìœ¼ë¡œ í—ˆìš©)
13) duration ë‚´ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì½ì„ ìˆ˜ ìˆëŠ” ë¶„ëŸ‰ìœ¼ë¡œ ì‘ì„±
14) í™”ë©´ì— ì—†ëŠ” ì •ë³´ ì°½ì‘ ê¸ˆì§€

ì •ë³´ í™œìš© ë²”ìœ„
15) ì£¼ë³€ ëŒ€í™” ì „í›„ ë§¥ë½ì„ ì°¸ê³ í•´ ì¥ë©´ì˜ ì´í•´ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìœ¼ë‚˜
    ìƒˆë¡œìš´ ì‚¬ê±´, ì¸ë¬¼ ì •ë³´ëŠ” ì¶”ê°€ ê¸ˆì§€
16) ì¹´ë©”ë¼ ì›€ì§ì„(ì¤Œ, íŒ¨ë‹), ì†Œí’ˆ ì‚¬ìš©, ëª¸ì§“Â·í–‰ë™Â·í‘œì • ë“±
    ì‹œê°ì  ë³€í™”ëŠ” ì ê·¹ì ìœ¼ë¡œ ê¸°ìˆ 

ì¶œë ¥ í˜•ì‹
17) JSON ì™¸ ë‹¤ë¥¸ í…ìŠ¤íŠ¸, ì£¼ì„, í—¤ë” ì ˆëŒ€ ê¸ˆì§€
18) audio_descriptions í•­ëª©ì€ ë°˜ë“œì‹œ ë°°ì—´ í˜•íƒœë¡œ ì œê³µ (ê¸¸ì´ 1 ê¸ˆì§€)

[ì¶œë ¥ JSON ìƒ˜í”Œ]
"audio_descriptions": [
  {
    "start_time": "0:03.5",
    "end_time": "0:06.1",
    "duration_sec": 2.6,
    "description": "ì¸ë¬¼ì´ ì»¤í”¼ì”ì„ ë“¤ì–´ ì…ì— ê°€ì ¸ê°€ê³  ì¡°ìš©íˆ í•œ ëª¨ê¸ˆ ë§ˆì‹ ë‹¤."
  }
]
"""

PROMPT_STT_KO = """
**ë¯¸ì…˜(MISSION):**
ì…ë ¥ ë¹„ë””ì˜¤ë¥¼ ë¶„ì„í•˜ì—¬ ì˜ìƒ ë‚´ ëª¨ë“  ìŒì„± ëŒ€ì‚¬ì™€ ì‚¬ìš´ë“œë¥¼ JSONìœ¼ë¡œ ê¸°ë¡í•˜ì‹œì˜¤.
ë¬´ìŒ êµ¬ê°„(2.5ì´ˆ ì´ìƒ)ê³¼ í•¨ê»˜ Audio Descriptionì„ ìƒì„±.
JSON í˜•ì‹ë§Œ ì¶œë ¥.

ì˜ˆì‹œ:
{
  "full_transcript": [
    {"time": "0:01.2", "speaker": "í™”ì1", "text": "ëŒ€ì‚¬ ë‚´ìš©"},
    {"time": "0:06.6", "speaker": "[Sound]", "text": "ì°¨ ë¬¸ ë‹«íˆëŠ” ì†Œë¦¬"}
  ]
}
"""

FINAL_PROMPT_KO = """
ì…ë ¥:
1) ì˜ìƒ ë©”íƒ€ë°ì´í„° â€” ì¥ë©´ ìš”ì†Œ, ë“±ì¥ì¸ë¬¼, ì†Œí’ˆ, ì‹œê°ì  í¬ì»¤ìŠ¤, ë³€í™” ì •ë³´
2) ê¸°ì¡´ Audio Description â€” ë¬´ìŒ êµ¬ê°„ ì¤‘ì‹¬
3) STT ê²°ê³¼ â€” ë°œí™” ë° ì£¼ìš” ì†Œë¦¬ ì „ì²´ ê¸°ë¡

ëª©í‘œ:
- Core AD, ë©”íƒ€ë°ì´í„°, STT ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… Audio Description JSON ìƒì„±
- ë¬´ìŒ êµ¬ê°„ ë™ì•ˆ í™”ë©´ì—ì„œ í™•ì¸ ê°€ëŠ¥í•œ ì‹œê°ì  ì„¸ë¶€ ì •ë³´ ë¬˜ì‚¬ (í–‰ë™, ì†Œí’ˆ, í‘œì •, ì¹´ë©”ë¼ ì›€ì§ì„ ë“±)
- ë©”íƒ€ë°ì´í„° í™œìš©í•´ ë§¥ë½ í’ë¶€í™” ë° ëˆ„ë½ ìš”ì†Œ ë³´ì™„
- STT ë°œí™”ëŠ” ë§¥ë½ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ í™œìš©, ì§§ì€ ì˜ë¯¸ ì—†ëŠ” ê°íƒ„ì‚¬ë‚˜ íš¨ê³¼ìŒ ë¬´ì‹œ
- ì¤‘ë³µ ì œê±°, duration_sec ë‚´ ìì—°ìŠ¤ëŸ½ê²Œ ì½íˆë„ë¡ ë¬¸ì¥ ì‘ì„±
- start_time, end_time, duration_sec ì •í™•íˆ ìœ ì§€
- í™”ë©´ì—ì„œ í™•ì¸ ê°€ëŠ¥í•œ ì •ë³´ë§Œ ì‚¬ìš©, ì¶”ì¸¡ ê¸ˆì§€
- ì¶œë ¥ì€ ë°˜ë“œì‹œ JSONë§Œ

ê·œì¹™:
1) Core ADì—ì„œ ëˆ„ë½ëœ ìš”ì†ŒëŠ” ë©”íƒ€ë°ì´í„°/ì‹œê° í¬ì»¤ìŠ¤ì—ì„œ í™•ì¸ ê°€ëŠ¥í•˜ë©´ ë°˜ë“œì‹œ í¬í•¨
2) STT ë¬´ìŒ êµ¬ê°„ì„ ì°¸ì¡°í•´ Core AD ë³´ì™„
3) ë°˜ë³µ ë‚´ìš© í†µí•©, í•µì‹¬ ì •ë³´ë§Œ ìœ ì§€
4) ì¶”ì • ê°ì •ì´ë‚˜ ë‚´ë©´ ë¬˜ì‚¬ ê¸ˆì§€
5) **ë°œí™”ê°€ ì—†ëŠ” êµ¬ê°„ â‰¥2.5ì´ˆì— ëŒ€í•´ì„œë§Œ description ìƒì„±**
6) ì˜ë¯¸ ì—†ëŠ” ê°íƒ„ì‚¬ë‚˜ ë¹„ì •ë³´ì„± íš¨ê³¼ìŒì€ ë¬´ìŒìœ¼ë¡œ ê°„ì£¼

ì¶œë ¥ ì˜ˆì‹œ:
"audio_descriptions": [
  {{
    "start_time": "0:03.5",
    "end_time": "0:06.1",
    "duration_sec": 2.6,
    "description": "í•œ ì‚¬ëŒì´ ì»¤í”¼ì”ì„ ë“¤ì–´ í•œ ëª¨ê¸ˆ ë§ˆì‹ ë‹¤."
  }}
]

[ì˜ìƒ ë©”íƒ€ë°ì´í„°]
{metadata}

[ê¸°ì¡´ AD]
{core_ad}

[STT ê²°ê³¼]
{stt_result}
"""

COMPRESS_PROMPT_KO = """
ë‹¹ì‹ ì€ í•œêµ­ì–´ Audio Description ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ ì„¤ëª…ì„ ì£¼ì–´ì§„ ì‹œê°„ ì•ˆì— ë§ê²Œ ì••ì¶•í•˜ì„¸ìš”.
í•„ìˆ˜ ì¡°ê±´:
- 3ì¸ì¹­ ê°ê´€ì  ì„œìˆ 
- ì™„ì „í•œ ë¬¸ì¥ í˜•íƒœ ("~ë‹¤.")
- í—ˆìš©ëœ ê¸€ì ìˆ˜ë¥¼ ë°˜ë“œì‹œ ì´ˆê³¼í•˜ì§€ ì•ŠëŠ”ë‹¤
- í•µì‹¬ì ì¸ ì‹œê° ì •ë³´ë§Œ ìœ ì§€í•œë‹¤
- ë³´ì´ì§€ ì•ŠëŠ” ìƒê°, ì¶”ì¸¡ì€ í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤
- í•œ ë¬¸ì¥ë§Œ ì¶œë ¥í•œë‹¤
- ì¶œë ¥ì€ ì˜¤ì§ ìµœì¢… ë¬¸ì¥ë§Œ!

ì›ë³¸ ì„¤ëª…: "{description}"

ì œí•œ ì •ë³´:
- í—ˆìš© ê¸€ì ìˆ˜: {max_chars}ì
"""

# ==============================================================================
# Prompts - English (synced with jack_ad_en.py)
# ==============================================================================
PROMPT_METADATA_EN = """
You are a video story analyst.

Goal:
Extract visually verifiable core metadata from the video and output it
as JSON to support high-quality Audio Description production.

Rules:
- Character names must be included only when they are visually identifiable.
- If a name cannot be determined from the video, use generic labels such as "speaker".
- Only use the title if it is explicitly shown; no assumptions.
- Do NOT generate any information that cannot be visually confirmed.
- Emotional or internal state descriptions are allowed only when there is a clear visual basis (e.g., facial expression).
- Output must be ONLY JSON â€” no additional text, comments, or explanation.

Output format (strictly unchanged):
{
  "video_title": "Include only if visibly confirmed, otherwise null",
  "overall_summary": "Concise visual summary of the entire video",
  "scenes": [
    {
      "scene_id": "Scene-1",
      "start_time": "0:00.0",
      "end_time": "0:05.8",
      "summary": "Summary of visually essential story information",
      "characters": [
        {
          "id": "char_1",
          "name": "Visually confirmed name (null if unknown)",
          "appearance": "Visible physical features",
          "visible_emotion": "Emotion based on clear facial expression only"
        }
      ],
      "visible_actions": [
        "Major clearly visible actions"
      ],
      "relationships": [
        "Only relationships with clear visual evidence"
      ],
      "visual_focus": "Primary visual focus of the scene"
    }
  ]
}
"""

PROMPT_AD_EN = """
Your primary objective is to accurately detect all silent segments in the video
based on audio analysis, excluding any portion containing dialogue or speech.

[RULES]

Silent Segment Detection
1) Exclude any segment containing speech, narration, or human vocal sounds
2) Detect every silent segment lasting at least 2.5 seconds and include them in a JSON array
3) Segments shorter than 2.5 seconds may be merged only if they naturally connect with adjacent silent regions
4) Do not select, shorten, or omit any valid silent segment
5) Do not modify or invent start/end times

Timestamps
6) Use the exact start_time and end_time of each silent segment
7) duration_sec must be calculated precisely as end_time - start_time
8) Timestamps must follow chronological order without overlap or missing time

Audio Description Creation
9) Describe only the essential on-screen visual information occurring during silence
10) Metadata (characters, setting, context) may be used only if visually verified
11) If multiple visual changes occur, avoid squeezing them into a single sentence â€” prioritize clarity
12) No emotional or internal state assumptions (only describe visible facial expressions or actions)
13) Ensure the description length is readable within the segment duration
14) Never invent details not seen on screen

Information Use
15) Spoken content before/after the silent segment may be referenced only to enhance clarity,
    but do not introduce new events or characters not visually confirmed
16) Actively include visible changes such as camera movement (zoom, panning),
    gestures, facial expressions, and interaction with objects

Output Format
17) Output must be strictly JSON â€” no explanations, comments, or headers
18) "audio_descriptions" must be an array containing multiple items (not length 1)

[OUTPUT JSON SAMPLE]
"audio_descriptions": [
  {
    "start_time": "0:03.5",
    "end_time": "0:06.1",
    "duration_sec": 2.6,
    "description": "A character lifts a coffee mug and quietly takes a sip."
  }
]
"""

PROMPT_STT_EN = """
**MISSION:**
Analyze the given video and extract all spoken dialogue and relevant sound events.
Record them in JSON format.
Include silent segments longer than 2.5 seconds as well.
Output must be JSON only.

Example:
{
  "full_transcript": [
    {"time": "0:01.2", "speaker": "Speaker 1", "text": "Dialogue content"},
    {"time": "0:06.6", "speaker": "[Sound]", "text": "Car door closing"}
  ]
}
"""

FINAL_PROMPT_EN = """
INPUT:
1) Video Metadata â€” scene elements, characters, objects, visual focus, changes
2) Core AD â€” existing Audio Description for silent segments
3) STT Result â€” transcript of spoken lines and significant sounds

OBJECTIVE:
- Generate FINAL Audio Description JSON by integrating Core AD, Metadata, and STT
- Describe visual details during silent segments (actions, props, expressions, camera motion)
- Use Metadata to enrich context and recover missing elements
- Use STT dialogue only for context; ignore short meaningless exclamations or non-informative sounds
- Remove redundancy, write readable sentences fitting within duration_sec
- Maintain exact start_time, end_time, duration_sec
- Screen-visible info only, no guesses
- Output strictly JSON

RULES:
1) Include details missing in Core AD if visible in Metadata or visual focus
2) Reference STT silent sections to complement Core AD
3) Merge repeated content, keep essentials
4) No inferred emotions or inner thoughts
5) Generate descriptions **only for segments without spoken dialogue â‰¥2.5 seconds**
6) Treat meaningless exclamations or non-informative sounds as silence

OUTPUT EXAMPLE:
"audio_descriptions": [
  {{
    "start_time": "0:03.5",
    "end_time": "0:06.1",
    "duration_sec": 2.6,
    "description": "A person lifts a coffee cup and takes a sip."
  }}
]

[VIDEO METADATA]
{metadata}

[CORE AD]
{core_ad}

[STT RESULT]
{stt_result}
"""

COMPRESS_PROMPT_EN = """
You are an expert in Audio Description.
Respond only in English.

Compress the following description to fit within the given time.
Required conditions:
- Third-person objective narration
- Must be a complete sentence ending with a period (.)
- Must not exceed the allowed character count
- Keep only the essential visual information
- Do not include unseen thoughts or assumptions
- Output exactly one sentence, nothing else

Original description: "{description}"
Restriction:
- Allowed character count: {max_chars} characters
"""


# ==============================================================================
# Async Helpers
# ==============================================================================
def get_gemini_client():
    """Get Gemini client with API key from environment."""
    api_key = os.environ.get("GEMINI_API_KEY") or os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GEMINI_API_KEY or GOOGLE_API_KEY environment variable is required")
    return genai.Client(api_key=api_key)


async def run_async(func, *args, **kwargs):
    """Run a synchronous function in thread pool."""
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(
        executor, lambda: func(*args, **kwargs)
    )


async def gemini_call(client, contents, model, temp=0, retries=3, delay=5):
    """Call Gemini API with retries.
    
    Config synced with jack_ad_ko.py:
    - temperature: 0 (default)
    - top_k: 1
    - top_p: 0.00001 (ê²°ì •ì  ì¶œë ¥)
    - thinking_config: {"thinking_budget": 8192}
    - system_instruction: "ì´ì „ api ì‚¬ìš©ìœ¼ë¡œ ì¸í•œ ëŒ€í™” ì ˆëŒ€ ì°¸ì¡° ê¸ˆì§€"
    """
    for attempt in range(retries):
        try:
            # Build config synced with jack_ad_ko.py BASE_CONFIG
            config_kwargs = {
                "temperature": temp,
                "system_instruction": "ì´ì „ api ì‚¬ìš©ìœ¼ë¡œ ì¸í•œ ëŒ€í™” ì ˆëŒ€ ì°¸ì¡° ê¸ˆì§€",
                "top_k": 1,
                "top_p": 0.00001,  # Deterministic output
                "thinking_config": {"thinking_budget": 8192},
            }
            
            response = await run_async(
                client.models.generate_content,
                model=model,
                contents=contents,
                config=config_kwargs,
            )
            
            if response is None:
                raise ValueError("Gemini returned None response object")
            
            response_text = response.text
            if response_text is None or not response_text.strip():
                raise ValueError("Gemini returned empty response")
            
            return response_text
            
        except (ServerError, Exception) as e:
            error_msg = str(e)
            if attempt < retries - 1:
                logger.warning(f"[Jack] Gemini retry {attempt+1}/{retries}: {error_msg}")
                await asyncio.sleep(delay)
            else:
                raise Exception(f"Gemini failed after {retries} retries: {error_msg}")


async def wait_for_file_active(client, file_obj, timeout=120, interval=3):
    """Wait for uploaded file to become active."""
    start = time.time()
    while time.time() - start < timeout:
        f = await run_async(client.files.get, name=file_obj.name)
        if f.state == "ACTIVE":
            logger.info(f"[Jack] File {file_obj.name} is now ACTIVE")
            return f
        logger.info(f"[Jack] Waiting for file to become ACTIVE: {file_obj.name}")
        await asyncio.sleep(interval)
    raise TimeoutError(f"File {file_obj.name} did not become ACTIVE within {timeout}s")


# ==============================================================================
# JSON Extraction
# ==============================================================================
def extract_json_from_response(text: str) -> Dict:
    """Extract JSON from API response text."""
    if not text:
        raise ValueError("Empty response text")
    
    text = text.strip()
    
    # If already pure JSON
    if text.startswith("{") and text.endswith("}"):
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass
    
    # Remove markdown code fences
    fence_match = re.search(r"```(?:json)?\s*(.+?)\s*```", text, re.DOTALL)
    if fence_match:
        candidate = fence_match.group(1).strip()
    else:
        candidate = text
    
    # Find JSON object
    match = re.search(r'\{[\s\S]*\}', candidate)
    if match:
        json_str = match.group(0)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            logger.warning(f"[Jack] JSON parse error: {e}")
            # Try to repair
            json_str = repair_json(json_str)
            return json.loads(json_str)
    
    raise ValueError("No JSON object found in response")


def repair_json(json_str: str) -> str:
    """Attempt to repair truncated or malformed JSON."""
    brace_count = json_str.count('{') - json_str.count('}')
    bracket_count = json_str.count('[') - json_str.count(']')
    
    json_str = re.sub(r',\s*$', '', json_str.rstrip())
    json_str += ']' * bracket_count
    json_str += '}' * brace_count
    
    return json_str


# ==============================================================================
# Stage2: Duration-based compression (TTS time-based)
# ==============================================================================
async def stage2_compress_description_async(
    client,
    ad_segments: List[Dict],
    lang: str = "ko",
    chars_per_sec: float = None
) -> List[Dict]:
    """
    Stage2: Compress descriptions to fit within TTS duration.
    
    Args:
        client: Gemini client
        ad_segments: List of AD segments with description, duration_sec
        lang: Language ('ko' or 'en')
        chars_per_sec: Characters per second rate (default: 7.0 for ko, 15.0 for en)
    
    Returns:
        List of compressed AD segments with metadata
    """
    if chars_per_sec is None:
        chars_per_sec = 7.0 if lang == "ko" else 15.0
    
    compress_prompt_template = COMPRESS_PROMPT_KO if lang == "ko" else COMPRESS_PROMPT_EN
    compressed = []
    
    for seg in ad_segments:
        description = seg.get("description", "")
        duration_sec = seg.get("duration_sec", 0)
        
        current_chars = len(description)
        max_chars = max(1, int(duration_sec * chars_per_sec))
        
        if current_chars > max_chars and description:
            # Need compression
            prompt = compress_prompt_template.format(
                description=description,
                max_chars=max_chars
            )
            
            try:
                comp = await gemini_call(
                    client,
                    [types.Part(text=prompt)],
                    GEMINI_MODEL_TEXT,
                    temp=0.2
                )
                
                if comp and isinstance(comp, str):
                    comp = comp.strip().splitlines()[0].strip()
                else:
                    comp = description
                    
            except Exception as e:
                logger.warning(f"[Jack] Stage2 compression failed: {e}")
                comp = description
            
            final_chars = len(comp)
            
            compressed.append({
                "start_time": seg.get("start_time"),
                "end_time": seg.get("end_time"),
                "duration_sec": duration_sec,
                "description": comp,
                "exceeds_limit": True,
                "allowed_chars": max_chars,
                "current_chars_before": current_chars,
                "current_chars_after": final_chars,
                "compressed_by_chars": True
            })
        else:
            compressed.append({
                **seg,
                "exceeds_limit": False,
                "allowed_chars": max_chars,
                "current_chars_before": current_chars,
                "current_chars_after": current_chars,
                "compressed_by_chars": False
            })
    
    return compressed


def stage2_compress_description_sync(
    ad_segments: List[Dict],
    lang: str = "ko",
    chars_per_sec: float = None
) -> List[Dict]:
    """
    Synchronous version of Stage2 compression (without Gemini calls).
    Fallback when async compression is not desired.
    """
    if chars_per_sec is None:
        chars_per_sec = 7.0 if lang == "ko" else 15.0
    
    compressed = []
    
    for seg in ad_segments:
        description = seg.get("description", "")
        duration_sec = seg.get("duration_sec", 0)
        
        current_chars = len(description)
        max_chars = max(1, int(duration_sec * chars_per_sec))
        
        compressed.append({
            **seg,
            "exceeds_limit": current_chars > max_chars,
            "allowed_chars": max_chars,
            "current_chars_before": current_chars,
            "current_chars_after": current_chars,
            "compressed_by_chars": False
        })
    
    return compressed


# ==============================================================================
# Main Processing
# ==============================================================================
async def process_video_async(video_path: str, lang: str = "ko") -> Tuple[Dict, List[Dict]]:
    """
    Process video with Jack AD pipeline.
    
    Pipeline:
    1. Upload video to Gemini
    2. Metadata extraction (temp=0)
    3. Core AD generation (temp=0)
    4. STT extraction (temp=0)
    5. Final Integration (temp=0)
    6. Stage2 compression
    
    Args:
        video_path: Path to video file
        lang: Language for output ('ko' or 'en')
    
    Returns:
        Tuple of (full_data, segments_list)
    """
    logger.info(f"[Jack] Starting AD generation for: {video_path}")
    logger.info(f"[Jack] Language: {lang}")
    
    client = get_gemini_client()
    
    # Select prompts based on language
    if lang == "ko":
        prompt_metadata = PROMPT_METADATA_KO
        prompt_ad = PROMPT_AD_KO
        prompt_stt = PROMPT_STT_KO
        final_prompt_template = FINAL_PROMPT_KO
    else:
        prompt_metadata = PROMPT_METADATA_EN
        prompt_ad = PROMPT_AD_EN
        prompt_stt = PROMPT_STT_EN
        final_prompt_template = FINAL_PROMPT_EN
    
    # =========================================================================
    # Step 1: Upload video
    # =========================================================================
    logger.info("[Jack] ğŸ“Œ Uploading video to Gemini...")
    with open(video_path, "rb") as f:
        uploaded = await run_async(
            client.files.upload,
            file=f,
            config={"mime_type": "video/mp4", "display_name": os.path.basename(video_path)},
        )
    
    uploaded = await wait_for_file_active(client, uploaded)
    video_ref = types.Part(
        file_data=types.FileData(file_uri=uploaded.uri, mime_type="video/mp4")
    )
    
    # =========================================================================
    # Step 2: Metadata extraction
    # =========================================================================
    logger.info("[Jack] === Metadata Step ===")
    contents_meta = [types.Part(text=prompt_metadata), video_ref]
    metadata_result = await gemini_call(client, contents_meta, GEMINI_MODEL_VISION, temp=0)
    logger.info(f"[Jack] Metadata length: {len(metadata_result)} chars")
    
    # =========================================================================
    # Step 3: Core AD generation
    # =========================================================================
    logger.info("[Jack] === Core AD Step ===")
    contents_ad = [types.Part(text=prompt_ad), video_ref]
    core_ad = await gemini_call(client, contents_ad, GEMINI_MODEL_VISION, temp=0)
    logger.info(f"[Jack] Core AD length: {len(core_ad)} chars")
    
    # =========================================================================
    # Step 4: STT extraction
    # =========================================================================
    logger.info("[Jack] === STT Step ===")
    contents_stt = [types.Part(text=prompt_stt), video_ref]
    stt_result = await gemini_call(client, contents_stt, GEMINI_MODEL_TEXT, temp=0)
    logger.info(f"[Jack] STT length: {len(stt_result)} chars")
    
    # =========================================================================
    # Step 5: Final Integration
    # =========================================================================
    logger.info("[Jack] === Final Integration Step ===")
    final_prompt = final_prompt_template.format(
        metadata=metadata_result,
        core_ad=core_ad,
        stt_result=stt_result,
    )
    
    final_response = await gemini_call(
        client,
        [types.Part(text=final_prompt)],
        GEMINI_MODEL_TEXT,
        temp=0
    )
    logger.info(f"[Jack] Final response length: {len(final_response)} chars")
    
    # Parse final result
    try:
        final_data = extract_json_from_response(final_response)
    except Exception as e:
        logger.error(f"[Jack] Failed to parse final response: {e}")
        logger.warning("[Jack] Falling back to core AD result")
        final_data = extract_json_from_response(core_ad)
    
    # Extract segments
    segments = final_data.get("audio_descriptions", [])
    if not segments and isinstance(final_data, list):
        segments = final_data
    
    # =========================================================================
    # Step 6: Stage2 compression
    # =========================================================================
    logger.info(f"[Jack] === Stage2 Compression (lang={lang}) ===")
    try:
        chars_per_sec = 7.0 if lang == "ko" else 15.0
        segments = await stage2_compress_description_async(
            client, segments, lang=lang, chars_per_sec=chars_per_sec
        )
        
        compressed_count = sum(1 for s in segments if s.get("compressed_by_chars", False))
        logger.info(f"[Jack] Stage2 completed: {compressed_count}/{len(segments)} segments compressed")
        
    except Exception as e:
        logger.warning(f"[Jack] Stage2 compression failed, using original: {e}")
        segments = stage2_compress_description_sync(segments, lang=lang)
    
    # =========================================================================
    # Format output
    # =========================================================================
    def parse_time_string(time_str) -> float:
        """Parse time string like '0:04.0' or '1:30.5' to seconds."""
        try:
            if isinstance(time_str, (int, float)):
                return float(time_str)
            if not time_str:
                return 0.0
            time_str = str(time_str).strip()
            parts = time_str.split(':')
            if len(parts) == 2:
                return float(parts[0]) * 60 + float(parts[1])
            if len(parts) == 3:
                return float(parts[0]) * 3600 + float(parts[1]) * 60 + float(parts[2])
            return float(time_str)
        except (ValueError, AttributeError):
            return 0.0
    
    formatted_segments = []
    for idx, seg in enumerate(segments, start=1):
        start_time = seg.get("start_time") or seg.get("start") or "0:00.0"
        end_time = seg.get("end_time") or seg.get("end") or "0:00.0"
        
        formatted_seg = {
            "id": idx,
            "start": parse_time_string(start_time),
            "end": parse_time_string(end_time),
            "text": seg.get("description") or seg.get("text") or "",
        }
        
        # Include compression metadata if available
        if seg.get("compressed_by_chars"):
            formatted_seg["_compression"] = {
                "exceeds_limit": seg.get("exceeds_limit", False),
                "allowed_chars": seg.get("allowed_chars"),
                "chars_before": seg.get("current_chars_before"),
                "chars_after": seg.get("current_chars_after"),
            }
        
        formatted_segments.append(formatted_seg)
    
    logger.info(f"[Jack] Generated {len(formatted_segments)} AD segments")
    
    # Cleanup uploaded file
    try:
        await run_async(client.files.delete, name=uploaded.name)
        logger.info(f"[Jack] Cleaned up uploaded file: {uploaded.name}")
    except Exception as e:
        logger.warning(f"[Jack] Failed to cleanup file: {e}")
    
    return {"audio_descriptions": formatted_segments}, formatted_segments


def generate_ad_for_video(video_path: str, lang: str = "ko") -> Tuple[Dict, List[Dict]]:
    """
    Synchronous wrapper for async processing.
    
    Args:
        video_path: Path to video file
        lang: Language for output ('ko' or 'en')
    
    Returns:
        Tuple of (full_data, segments_list)
    """
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None
    
    if loop and loop.is_running():
        import nest_asyncio
        nest_asyncio.apply()
        return asyncio.run(process_video_async(video_path, lang))
    else:
        return asyncio.run(process_video_async(video_path, lang))


def save_ad_json(video_id: str, data: Any, output_dir: str) -> str:
    """Save AD data to JSON file."""
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"{video_id}.ad.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    logger.info(f"[Jack] JSON saved to: {output_path}")
    return output_path


# ==============================================================================
# CLI Entry Point
# ==============================================================================
def main():
    """CLI entry point for Jack AD generation."""
    parser = argparse.ArgumentParser(description="Jack AD Generation")
    parser.add_argument("video_path", help="Path to input video file")
    parser.add_argument("--lang", choices=["ko", "en"], default="ko", help="Output language")
    parser.add_argument("--output", help="Output directory for JSON")
    parser.add_argument("--video-id", help="Video ID for output filename")
    
    args = parser.parse_args()
    
    logging.basicConfig(
        level=logging.INFO,
        format="[%(levelname)s] %(message)s"
    )
    
    try:
        full_data, segments = generate_ad_for_video(args.video_path, args.lang)
        
        if args.output and args.video_id:
            save_ad_json(args.video_id, full_data, args.output)
        
        result = {
            "success": True,
            "segments": segments,
            "model": "jack"
        }
        print(json.dumps(result, ensure_ascii=False))
        
    except Exception as e:
        import traceback
        result = {
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc(),
            "model": "jack"
        }
        print(json.dumps(result, ensure_ascii=False))
        exit(1)


if __name__ == "__main__":
    main()
