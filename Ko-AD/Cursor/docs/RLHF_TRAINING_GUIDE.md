# Ko-AD RLHF í•™ìŠµ ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” Ko-AD ì‹œìŠ¤í…œì—ì„œ ìˆ˜ì§‘ëœ ì‚¬ìš©ì í‰ê°€ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ í™”ë©´í•´ì„¤(Audio Description) ìƒì„± ëª¨ë¸ì„ RLHF(Reinforcement Learning from Human Feedback) ë°©ì‹ìœ¼ë¡œ ê°œì„ í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

---

## ëª©ì°¨

1. [ìˆ˜ì§‘ ë°ì´í„° êµ¬ì¡°](#1-ìˆ˜ì§‘-ë°ì´í„°-êµ¬ì¡°)
2. [ë°ì´í„° ì „ì²˜ë¦¬](#2-ë°ì´í„°-ì „ì²˜ë¦¬)
3. [RLHF í•™ìŠµ ë°ì´í„° ë³€í™˜](#3-rlhf-í•™ìŠµ-ë°ì´í„°-ë³€í™˜)
4. [í•™ìŠµ íŒŒì´í”„ë¼ì¸](#4-í•™ìŠµ-íŒŒì´í”„ë¼ì¸)
5. [ëª¨ë¸ Fine-tuning ë°©ë²•](#5-ëª¨ë¸-fine-tuning-ë°©ë²•)
6. [í‰ê°€ ë° ê²€ì¦](#6-í‰ê°€-ë°-ê²€ì¦)

---

## 1. ìˆ˜ì§‘ ë°ì´í„° êµ¬ì¡°

### 1.1 ì €ì¥ ìœ„ì¹˜

```
Cursor/server/storage/ratings/
â”œâ”€â”€ {videoId}_ratings.json          # ì›ë³¸ ë²„ì „ í‰ê°€
â”œâ”€â”€ {videoId}_edited_ratings.json   # í¸ì§‘ ë²„ì „ í‰ê°€
â””â”€â”€ ...
```

### 1.2 JSON ìŠ¤í‚¤ë§ˆ

```json
{
  "videoId": "uuid-string",
  "videoInfo": {
    "fileName": "example_video.mp4",
    "duration": 180.5,
    "width": 1920,
    "height": 1080
  },
  "segments": [
    {
      "id": 1,
      "start": 5.0,
      "end": 12.5,
      "text": "í™”ë©´í•´ì„¤ í…ìŠ¤íŠ¸...",
      "rating": "like"
    }
  ],
  "version": "original",
  "createdAt": "2025-12-03T10:30:00.000Z",
  "updatedAt": "2025-12-03T10:35:00.000Z"
}
```

### 1.3 Rating ê°’ ì˜ë¯¸

| Rating | ì˜ë¯¸ | RLHF í™œìš© |
|--------|------|-----------|
| `like` (ğŸ‘) | ì¢‹ì€ í™”ë©´í•´ì„¤ | Positive sample (reward = +1) |
| `dislike` (ğŸ‘) | ê°œì„  í•„ìš” | Negative sample (reward = -1) |
| `neutral` | í‰ê°€ ì•ˆí•¨ | í•™ìŠµ ì œì™¸ ë˜ëŠ” ì¤‘ë¦½ (reward = 0) |

### 1.4 ë²„ì „ë³„ ë°ì´í„° ì˜ë¯¸

- **original**: AIê°€ ìƒì„±í•œ ì›ë³¸ í™”ë©´í•´ì„¤ì— ëŒ€í•œ í‰ê°€
- **edited**: ì‚¬ìš©ìê°€ ìˆ˜ì •í•œ í™”ë©´í•´ì„¤ (ìˆ˜ì •ëœ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ìë™ìœ¼ë¡œ `like` ì²˜ë¦¬)

---

## 2. ë°ì´í„° ì „ì²˜ë¦¬

### 2.1 ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸

```python
# scripts/collect_ratings.py
import os
import json
from pathlib import Path
from typing import List, Dict, Any

RATINGS_DIR = Path("Cursor/server/storage/ratings")

def collect_all_ratings() -> List[Dict[str, Any]]:
    """ëª¨ë“  í‰ê°€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    all_ratings = []
    
    for json_file in RATINGS_DIR.glob("*.json"):
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            all_ratings.append(data)
    
    return all_ratings

def filter_rated_segments(ratings_list: List[Dict]) -> List[Dict]:
    """í‰ê°€ëœ ì„¸ê·¸ë¨¼íŠ¸ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤ (neutral ì œì™¸)."""
    filtered = []
    
    for rating_data in ratings_list:
        video_id = rating_data['videoId']
        version = rating_data.get('version', 'original')
        
        for segment in rating_data['segments']:
            if segment['rating'] != 'neutral':
                filtered.append({
                    'video_id': video_id,
                    'version': version,
                    'segment_id': segment['id'],
                    'start_time': segment['start'],
                    'end_time': segment['end'],
                    'text': segment['text'],
                    'rating': segment['rating'],
                    'reward': 1 if segment['rating'] == 'like' else -1
                })
    
    return filtered

if __name__ == "__main__":
    ratings = collect_all_ratings()
    filtered = filter_rated_segments(ratings)
    
    # í†µê³„ ì¶œë ¥
    likes = sum(1 for s in filtered if s['rating'] == 'like')
    dislikes = sum(1 for s in filtered if s['rating'] == 'dislike')
    
    print(f"ì´ í‰ê°€ ë°ì´í„°: {len(filtered)}")
    print(f"  - Like: {likes}")
    print(f"  - Dislike: {dislikes}")
    
    # í•™ìŠµ ë°ì´í„°ë¡œ ì €ì¥
    with open('training_data.json', 'w', encoding='utf-8') as f:
        json.dump(filtered, f, ensure_ascii=False, indent=2)
```

### 2.2 Preference Pair ìƒì„±

RLHFì—ì„œëŠ” "ì„ í˜¸ ìŒ(Preference Pair)"ì´ í•„ìš”í•©ë‹ˆë‹¤. í¸ì§‘ëœ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ìƒì„±í•©ë‹ˆë‹¤.

```python
# scripts/create_preference_pairs.py
import json
from pathlib import Path
from typing import List, Dict, Tuple

def create_preference_pairs(ratings_dir: Path) -> List[Dict]:
    """ì›ë³¸-í¸ì§‘ ìŒì„ ë¹„êµí•˜ì—¬ Preference Pairë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    preference_pairs = []
    
    # ë¹„ë””ì˜¤ë³„ë¡œ ì›ë³¸/í¸ì§‘ ë²„ì „ ë§¤ì¹­
    video_ratings = {}
    
    for json_file in ratings_dir.glob("*.json"):
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            video_id = data['videoId']
            version = data.get('version', 'original')
            
            if video_id not in video_ratings:
                video_ratings[video_id] = {}
            video_ratings[video_id][version] = data
    
    # Preference Pair ìƒì„±
    for video_id, versions in video_ratings.items():
        if 'original' not in versions or 'edited' not in versions:
            continue
        
        original = versions['original']
        edited = versions['edited']
        
        # ì„¸ê·¸ë¨¼íŠ¸ë³„ ë¹„êµ
        orig_segments = {s['id']: s for s in original['segments']}
        edit_segments = {s['id']: s for s in edited['segments']}
        
        for seg_id in orig_segments:
            if seg_id not in edit_segments:
                continue
            
            orig_seg = orig_segments[seg_id]
            edit_seg = edit_segments[seg_id]
            
            # í…ìŠ¤íŠ¸ê°€ ë‹¤ë¥¸ ê²½ìš° = ì‚¬ìš©ìê°€ ìˆ˜ì •í•¨ = í¸ì§‘ ë²„ì „ ì„ í˜¸
            if orig_seg['text'] != edit_seg['text']:
                preference_pairs.append({
                    'video_id': video_id,
                    'segment_id': seg_id,
                    'start_time': orig_seg['start'],
                    'end_time': orig_seg['end'],
                    'chosen': edit_seg['text'],      # ì„ í˜¸ë¨ (í¸ì§‘ëœ ë²„ì „)
                    'rejected': orig_seg['text'],    # ê±°ë¶€ë¨ (ì›ë³¸ ë²„ì „)
                    'chosen_rating': edit_seg.get('rating', 'like'),
                    'rejected_rating': orig_seg.get('rating', 'dislike')
                })
    
    return preference_pairs

if __name__ == "__main__":
    ratings_dir = Path("Cursor/server/storage/ratings")
    pairs = create_preference_pairs(ratings_dir)
    
    print(f"ìƒì„±ëœ Preference Pairs: {len(pairs)}")
    
    with open('preference_pairs.json', 'w', encoding='utf-8') as f:
        json.dump(pairs, f, ensure_ascii=False, indent=2)
```

---

## 3. RLHF í•™ìŠµ ë°ì´í„° ë³€í™˜

### 3.1 OpenAI Fine-tuning í˜•ì‹

```python
# scripts/convert_to_openai_format.py
import json

def convert_to_openai_format(preference_pairs: list) -> list:
    """OpenAI Fine-tuning JSONL í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    training_examples = []
    
    for pair in preference_pairs:
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = """ë‹¹ì‹ ì€ ì‹œê°ì¥ì• ì¸ì„ ìœ„í•œ í™”ë©´í•´ì„¤(Audio Description) ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì˜ìƒì˜ íŠ¹ì • êµ¬ê°„ì— ëŒ€í•´ ì ì ˆí•œ í™”ë©´í•´ì„¤ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
í™”ë©´í•´ì„¤ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ë©°, ì‹œê°ì  ì •ë³´ë¥¼ ì²­ê°ì ìœ¼ë¡œ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤."""

        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ (ì˜ìƒ êµ¬ê°„ ì •ë³´)
        user_prompt = f"""ë‹¤ìŒ ì˜ìƒ êµ¬ê°„ì— ëŒ€í•œ í™”ë©´í•´ì„¤ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

êµ¬ê°„: {pair['start_time']:.1f}ì´ˆ ~ {pair['end_time']:.1f}ì´ˆ

ì¢‹ì€ í™”ë©´í•´ì„¤ì˜ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì„¸ìš”."""

        # ì„ í˜¸ëœ ì‘ë‹µ (chosen)
        training_examples.append({
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
                {"role": "assistant", "content": pair['chosen']}
            ]
        })
    
    return training_examples

def save_as_jsonl(examples: list, output_path: str):
    """JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    with open(output_path, 'w', encoding='utf-8') as f:
        for example in examples:
            f.write(json.dumps(example, ensure_ascii=False) + '\n')

if __name__ == "__main__":
    with open('preference_pairs.json', 'r', encoding='utf-8') as f:
        pairs = json.load(f)
    
    examples = convert_to_openai_format(pairs)
    save_as_jsonl(examples, 'openai_training.jsonl')
    print(f"ì €ì¥ ì™„ë£Œ: {len(examples)} examples")
```

### 3.2 DPO (Direct Preference Optimization) í˜•ì‹

```python
# scripts/convert_to_dpo_format.py
import json

def convert_to_dpo_format(preference_pairs: list) -> list:
    """DPO í•™ìŠµì„ ìœ„í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    dpo_examples = []
    
    for pair in preference_pairs:
        prompt = f"""[ì˜ìƒ êµ¬ê°„: {pair['start_time']:.1f}ì´ˆ ~ {pair['end_time']:.1f}ì´ˆ]
ì´ êµ¬ê°„ì— ëŒ€í•œ í™”ë©´í•´ì„¤ì„ ì‘ì„±í•˜ì„¸ìš”."""

        dpo_examples.append({
            "prompt": prompt,
            "chosen": pair['chosen'],
            "rejected": pair['rejected']
        })
    
    return dpo_examples

if __name__ == "__main__":
    with open('preference_pairs.json', 'r', encoding='utf-8') as f:
        pairs = json.load(f)
    
    dpo_data = convert_to_dpo_format(pairs)
    
    with open('dpo_training.json', 'w', encoding='utf-8') as f:
        json.dump(dpo_data, f, ensure_ascii=False, indent=2)
    
    print(f"DPO ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(dpo_data)} examples")
```

### 3.3 Reward Model í•™ìŠµ ë°ì´í„°

```python
# scripts/create_reward_data.py
import json

def create_reward_model_data(filtered_segments: list) -> list:
    """Reward Model í•™ìŠµìš© ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    reward_data = []
    
    for seg in filtered_segments:
        reward_data.append({
            "text": seg['text'],
            "start_time": seg['start_time'],
            "end_time": seg['end_time'],
            "reward": seg['reward'],  # 1 (like) or -1 (dislike)
            "label": 1 if seg['reward'] > 0 else 0  # Binary classification
        })
    
    return reward_data

if __name__ == "__main__":
    with open('training_data.json', 'r', encoding='utf-8') as f:
        segments = json.load(f)
    
    reward_data = create_reward_model_data(segments)
    
    with open('reward_model_data.json', 'w', encoding='utf-8') as f:
        json.dump(reward_data, f, ensure_ascii=False, indent=2)
    
    print(f"Reward Model ë°ì´í„°: {len(reward_data)} samples")
```

---

## 4. í•™ìŠµ íŒŒì´í”„ë¼ì¸

### 4.1 ì „ì²´ RLHF íŒŒì´í”„ë¼ì¸

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ì‚¬ìš©ì í‰ê°€     â”‚  Ko-AD Upload/Editor í˜ì´ì§€
â”‚  (ğŸ‘/ğŸ‘)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  í‰ê°€ JSON ìˆ˜ì§‘  â”‚  storage/ratings/*.json
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ë°ì´í„° ì „ì²˜ë¦¬   â”‚  Preference Pairs ìƒì„±
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SFT  â”‚ â”‚ Reward    â”‚  1. Supervised Fine-tuning
â”‚       â”‚ â”‚ Model     â”‚  2. Reward Model í•™ìŠµ
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
    â”‚           â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PPO / DPO      â”‚  3. RL Fine-tuning
â”‚  Training       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ê°œì„ ëœ AD ëª¨ë¸  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 ì‹¤í–‰ ìˆœì„œ

```bash
# 1. ë°ì´í„° ìˆ˜ì§‘
python scripts/collect_ratings.py

# 2. Preference Pairs ìƒì„±
python scripts/create_preference_pairs.py

# 3. í˜•ì‹ ë³€í™˜ (ëª©ì ì— ë§ê²Œ ì„ íƒ)
python scripts/convert_to_openai_format.py  # OpenAI Fine-tuning
python scripts/convert_to_dpo_format.py     # DPO Training
python scripts/create_reward_data.py        # Reward Model

# 4. í•™ìŠµ ì‹¤í–‰
python scripts/train_model.py
```

---

## 5. ëª¨ë¸ Fine-tuning ë°©ë²•

### 5.1 OpenAI GPT Fine-tuning

```python
# scripts/finetune_openai.py
import openai
import os

openai.api_key = os.getenv("OPENAI_API_KEY")

# 1. í•™ìŠµ íŒŒì¼ ì—…ë¡œë“œ
def upload_training_file(file_path: str) -> str:
    with open(file_path, "rb") as f:
        response = openai.File.create(
            file=f,
            purpose="fine-tune"
        )
    return response.id

# 2. Fine-tuning ì‘ì—… ìƒì„±
def create_finetune_job(training_file_id: str, model: str = "gpt-3.5-turbo"):
    response = openai.FineTuningJob.create(
        training_file=training_file_id,
        model=model,
        hyperparameters={
            "n_epochs": 3,
            "batch_size": 4,
            "learning_rate_multiplier": 0.1
        }
    )
    return response

# 3. í•™ìŠµ ìƒíƒœ í™•ì¸
def check_finetune_status(job_id: str):
    return openai.FineTuningJob.retrieve(job_id)

if __name__ == "__main__":
    # í•™ìŠµ íŒŒì¼ ì—…ë¡œë“œ
    file_id = upload_training_file("openai_training.jsonl")
    print(f"ì—…ë¡œë“œëœ íŒŒì¼ ID: {file_id}")
    
    # Fine-tuning ì‹œì‘
    job = create_finetune_job(file_id)
    print(f"Fine-tuning Job ID: {job.id}")
    print(f"ìƒíƒœ: {job.status}")
```

### 5.2 Hugging Face DPO Training

```python
# scripts/train_dpo.py
from datasets import Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import DPOTrainer, DPOConfig
import json

def load_dpo_dataset(file_path: str) -> Dataset:
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return Dataset.from_list(data)

def train_dpo_model(
    model_name: str = "beomi/KoAlpaca-Polyglot-5.8B",
    output_dir: str = "./ko-ad-dpo-model"
):
    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    dataset = load_dpo_dataset("dpo_training.json")
    
    # DPO ì„¤ì •
    training_args = DPOConfig(
        output_dir=output_dir,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=5e-5,
        num_train_epochs=3,
        beta=0.1,  # DPO beta parameter
        logging_steps=10,
        save_steps=100,
        evaluation_strategy="steps",
        eval_steps=50,
    )
    
    # DPO Trainer ì´ˆê¸°í™”
    trainer = DPOTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        tokenizer=tokenizer,
    )
    
    # í•™ìŠµ ì‹œì‘
    trainer.train()
    
    # ëª¨ë¸ ì €ì¥
    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    print(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir}")

if __name__ == "__main__":
    train_dpo_model()
```

### 5.3 Reward Model í•™ìŠµ

```python
# scripts/train_reward_model.py
import torch
from torch import nn
from transformers import AutoModel, AutoTokenizer, Trainer, TrainingArguments
from datasets import Dataset
import json

class RewardModel(nn.Module):
    def __init__(self, base_model_name: str):
        super().__init__()
        self.base_model = AutoModel.from_pretrained(base_model_name)
        self.reward_head = nn.Linear(self.base_model.config.hidden_size, 1)
    
    def forward(self, input_ids, attention_mask):
        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden_state = outputs.last_hidden_state
        # [CLS] í† í°ì˜ hidden state ì‚¬ìš©
        cls_output = last_hidden_state[:, 0, :]
        reward = self.reward_head(cls_output)
        return reward

def load_reward_dataset(file_path: str) -> Dataset:
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return Dataset.from_list(data)

def train_reward_model(
    model_name: str = "klue/bert-base",
    output_dir: str = "./ko-ad-reward-model"
):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = RewardModel(model_name)
    
    dataset = load_reward_dataset("reward_model_data.json")
    
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            padding="max_length",
            truncation=True,
            max_length=256
        )
    
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=16,
        num_train_epochs=5,
        learning_rate=2e-5,
        logging_steps=50,
        save_steps=200,
        evaluation_strategy="steps",
        eval_steps=100,
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
    )
    
    trainer.train()
    trainer.save_model(output_dir)
    
    print(f"Reward Model ì €ì¥ ì™„ë£Œ: {output_dir}")

if __name__ == "__main__":
    train_reward_model()
```

---

## 6. í‰ê°€ ë° ê²€ì¦

### 6.1 ìë™ í‰ê°€ ì§€í‘œ

```python
# scripts/evaluate_model.py
from bert_score import score as bert_score
from nltk.translate.meteor_score import meteor_score
import json

def evaluate_ad_quality(predictions: list, references: list) -> dict:
    """í™”ë©´í•´ì„¤ í’ˆì§ˆì„ ìë™ í‰ê°€í•©ë‹ˆë‹¤."""
    
    # BERTScore
    P, R, F1 = bert_score(predictions, references, lang="ko")
    
    # METEOR Score
    meteor_scores = [
        meteor_score([ref.split()], pred.split())
        for pred, ref in zip(predictions, references)
    ]
    
    return {
        "bert_score_precision": P.mean().item(),
        "bert_score_recall": R.mean().item(),
        "bert_score_f1": F1.mean().item(),
        "meteor_score": sum(meteor_scores) / len(meteor_scores)
    }

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    with open('test_predictions.json', 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    predictions = [d['prediction'] for d in test_data]
    references = [d['reference'] for d in test_data]
    
    results = evaluate_ad_quality(predictions, references)
    print("í‰ê°€ ê²°ê³¼:")
    for metric, value in results.items():
        print(f"  {metric}: {value:.4f}")
```

### 6.2 A/B í…ŒìŠ¤íŠ¸ ì„¤ì •

```python
# Ko-AD ì‹œìŠ¤í…œì—ì„œ A/B í…ŒìŠ¤íŠ¸ ì„¤ì • ì˜ˆì‹œ
AB_TEST_CONFIG = {
    "enabled": True,
    "models": {
        "control": "gemini-2.0-flash",      # ê¸°ì¡´ ëª¨ë¸
        "treatment": "ko-ad-dpo-v1"          # Fine-tuned ëª¨ë¸
    },
    "traffic_split": 0.5,  # 50% ì‚¬ìš©ìì—ê²Œ ìƒˆ ëª¨ë¸ ì ìš©
    "metrics_to_track": [
        "like_rate",
        "dislike_rate", 
        "edit_rate",
        "time_to_approve"
    ]
}
```

### 6.3 ì§€ì†ì  ê°œì„  ë£¨í”„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ko-AD ì‹œìŠ¤í…œ                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Upload  â”‚ -> â”‚ Editor  â”‚ -> â”‚ Export  â”‚         â”‚
â”‚  â”‚ Page    â”‚    â”‚ Page    â”‚    â”‚         â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚       â”‚              â”‚                              â”‚
â”‚       â–¼              â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚   í‰ê°€ ë°ì´í„° ìˆ˜ì§‘        â”‚                       â”‚
â”‚  â”‚   (ğŸ‘/ğŸ‘/í¸ì§‘)           â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LLMOps íŒŒì´í”„ë¼ì¸                        â”‚
â”‚                                                      â”‚
â”‚  1. ì£¼ê°„ ë°ì´í„° ìˆ˜ì§‘ (ìë™í™”)                         â”‚
â”‚  2. ë°ì´í„° í’ˆì§ˆ ê²€ì¦                                  â”‚
â”‚  3. Preference Pair ìƒì„±                             â”‚
â”‚  4. ëª¨ë¸ ì¬í•™ìŠµ (DPO/PPO)                            â”‚
â”‚  5. A/B í…ŒìŠ¤íŠ¸ ë°°í¬                                  â”‚
â”‚  6. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§                                    â”‚
â”‚  7. ìƒˆ ëª¨ë¸ í”„ë¡œë•ì…˜ ì ìš©                             â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ë¶€ë¡: ìœ ìš©í•œ ë¦¬ì†ŒìŠ¤

### A. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬

```bash
# requirements-rlhf.txt
transformers>=4.35.0
datasets>=2.14.0
trl>=0.7.0          # DPO, PPO Trainer
peft>=0.5.0         # LoRA ì§€ì›
accelerate>=0.24.0
bitsandbytes>=0.41.0
bert-score>=0.3.13
nltk>=3.8.0
openai>=1.0.0
```

### B. ì°¸ê³  ë…¼ë¬¸

1. **RLHF**: "Training language models to follow instructions with human feedback" (InstructGPT)
2. **DPO**: "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
3. **PPO**: "Proximal Policy Optimization Algorithms"

### C. ë°ì´í„° í’ˆì§ˆ ê°€ì´ë“œë¼ì¸

- **ìµœì†Œ ë°ì´í„°ëŸ‰**: Preference Pair 1,000ê°œ ì´ìƒ ê¶Œì¥
- **ê· í˜•**: Like/Dislike ë¹„ìœ¨ ê· í˜• ìœ ì§€ (ì´ìƒì ìœ¼ë¡œ 40-60%)
- **ë‹¤ì–‘ì„±**: ë‹¤ì–‘í•œ ì˜ìƒ ì¥ë¥´, ê¸¸ì´, ì¥ë©´ ìœ í˜• í¬í•¨
- **ì¼ê´€ì„±**: í‰ê°€ ê¸°ì¤€ ì¼ê´€ì„± í™•ë³´ (ê°€ì´ë“œë¼ì¸ ì œê³µ)

---

## ë¬¸ì˜

Ko-AD RLHF í•™ìŠµì— ëŒ€í•œ ë¬¸ì˜ì‚¬í•­ì€ ì´ìŠˆë¥¼ í†µí•´ ë‚¨ê²¨ì£¼ì„¸ìš”.

