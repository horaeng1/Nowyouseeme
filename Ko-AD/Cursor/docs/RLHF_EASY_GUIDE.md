# 🎓 RLHF 쉽게 이해하기

> **"AI를 똑똒하게 만드는 방법을 쉽게 알아보자!"**

이 문서는 Ko-AD 시스템의 RLHF 학습 과정을 **누구나 이해할 수 있게** 설명합니다.

---

## 📚 목차

1. [RLHF가 뭐야?](#1-rlhf가-뭐야)
2. [우리가 하는 일 전체 그림](#2-우리가-하는-일-전체-그림)
3. [Step 1: 사람들의 의견 모으기](#3-step-1-사람들의-의견-모으기)
4. [Step 2: 데이터 정리하기](#4-step-2-데이터-정리하기)
5. [Step 3: AI 선생님 만들기 (Reward Model)](#5-step-3-ai-선생님-만들기-reward-model)
6. [Step 4: AI 학생 가르치기 (Fine-tuning)](#6-step-4-ai-학생-가르치기-fine-tuning)
7. [Step 5: 시험 보기 (평가)](#7-step-5-시험-보기-평가)
8. [학습 방법 비교](#8-학습-방법-비교)
9. [자주 묻는 질문](#9-자주-묻는-질문)

---

## 1. RLHF가 뭐야?

### 🤔 한 줄 설명

> **RLHF** = **R**einforcement **L**earning from **H**uman **F**eedback
> 
> 한국어로: **"사람 피드백으로 AI 가르치기"**

### 🎯 비유로 이해하기

**AI = 글짓기를 배우는 학생** 🧒

```
일반 AI 학습:
┌─────────────────────────────────────────────────┐
│  📖 교과서만 읽고 공부                            │
│                                                 │
│  학생: "교과서에 이렇게 써있으니까 이렇게 쓸게!"    │
│                                                 │
│  문제점: 교과서는 "정답"만 있고,                   │
│         "왜 이게 좋은지"는 안 알려줌              │
└─────────────────────────────────────────────────┘

RLHF 학습:
┌─────────────────────────────────────────────────┐
│  👨‍🏫 선생님이 직접 피드백 해줌                    │
│                                                 │
│  학생: "선생님, 이거 어때요?"                     │
│  선생님: "👍 이건 좋아! 이유는..."                │
│  선생님: "👎 이건 별로야. 왜냐면..."              │
│                                                 │
│  장점: 학생이 "왜" 좋은지 배울 수 있음!            │
└─────────────────────────────────────────────────┘
```

### 🎬 Ko-AD에서는?

```
┌────────────────────────────────────────────────────────┐
│  Ko-AD = 시각장애인을 위한 화면해설 만드는 AI          │
│                                                        │
│  예시)                                                 │
│  ┌──────────────────────────────────────────────┐     │
│  │ 🎥 영상: 남자가 문을 열고 들어온다              │     │
│  │                                              │     │
│  │ AI 화면해설 A: "남자가 등장한다"               │     │
│  │ AI 화면해설 B: "정장을 입은 중년 남성이         │     │
│  │               나무 문을 열고 방으로 들어온다"   │     │
│  │                                              │     │
│  │ 사용자 평가: B가 더 좋아! 👍                  │     │
│  └──────────────────────────────────────────────┘     │
│                                                        │
│  → AI가 "B 스타일"로 더 잘 쓰게 학습!                  │
└────────────────────────────────────────────────────────┘
```

### ✅ 왜 RLHF를 해야 하나요?

| 이유 | 설명 |
|------|------|
| 🎯 **사람 취향 반영** | 기술적으로 맞아도 사람이 싫어하면 소용없음 |
| 🔄 **지속적 개선** | 사용하면서 계속 좋아짐 |
| 🎨 **주관적 품질 학습** | "좋은 글"은 정답이 없음. 사람 피드백이 필수! |

---

## 2. 우리가 하는 일 전체 그림

### 🗺️ 전체 과정 (5단계)

```
        👤 사용자                          🤖 AI
           │                               │
           │                               │
   ┌───────▼───────┐                       │
   │ 1️⃣ 의견 모으기 │ ◄──── 사용자가 평가 ──┤
   │   (👍 / 👎)    │                       │
   └───────┬───────┘                       │
           │                               │
           ▼                               │
   ┌───────────────┐                       │
   │ 2️⃣ 데이터 정리 │ ◄── "좋은 것 vs 나쁜 것" 비교
   │  (Preference   │                       │
   │    Pairs)     │                       │
   └───────┬───────┘                       │
           │                               │
           ▼                               │
   ┌───────────────┐                       │
   │ 3️⃣ AI 선생님   │ ◄── "좋고 나쁨을 판단하는 AI"
   │  만들기       │                       │
   │ (Reward Model)│                       │
   └───────┬───────┘                       │
           │                               │
           ▼                               │
   ┌───────────────┐                       │
   │ 4️⃣ AI 학생     │ ──── 더 좋은 답변 ────►
   │  가르치기     │                       │
   │ (Fine-tuning) │                       │
   └───────┬───────┘                       │
           │                               │
           ▼                               │
   ┌───────────────┐                       │
   │ 5️⃣ 시험 보기   │ ◄── "진짜 좋아졌나?" 확인
   │   (평가)      │                       │
   └───────────────┘
```

---

## 3. Step 1: 사람들의 의견 모으기

### 📋 무엇을 하나요?

```
Ko-AD 앱에서 사용자가 화면해설에 평가를 남김:

┌─────────────────────────────────────────────┐
│  영상 구간: 0:15 ~ 0:23                      │
│  ─────────────────────────────────────────  │
│  AI 화면해설:                                │
│  "주인공이 창문 밖을 바라본다"                 │
│                                             │
│  이 화면해설 어때요?                          │
│                                             │
│  ┌─────┐    ┌─────┐                         │
│  │ 👍  │    │ 👎  │                         │
│  │좋아요│    │별로예│                         │
│  └─────┘    └─────┘                         │
│                                             │
│  또는 직접 수정: [편집하기]                    │
└─────────────────────────────────────────────┘
```

### 💾 저장되는 데이터

```json
{
  "영상이름": "드라마_1화.mp4",
  "구간들": [
    {
      "시작": 15.0,
      "끝": 23.0,
      "AI가_쓴_화면해설": "주인공이 창문 밖을 바라본다",
      "평가": "like"   ← 👍 눌렀음!
    },
    {
      "시작": 45.0,
      "끝": 52.0,
      "AI가_쓴_화면해설": "남자가 간다",
      "평가": "dislike"  ← 👎 눌렀음!
    }
  ]
}
```

### 🎯 왜 이걸 하나요?

| 이유 | 설명 |
|------|------|
| 📊 **학습 재료** | AI를 가르치려면 "뭐가 좋고 나쁜지" 알아야 함 |
| 🎓 **실제 선호도** | 이론이 아닌 **실제 사용자** 의견 |
| 🔄 **다양한 관점** | 여러 사람의 평가로 편향 줄임 |

### ✅ 장점 & ❌ 단점

| 장점 ✅ | 단점 ❌ |
|---------|---------|
| 실제 사용자 의견 반영 | 평가하는 데 시간 걸림 |
| 다양한 상황 커버 | 사람마다 기준이 다를 수 있음 |
| 지속적 수집 가능 | 초기에는 데이터가 적음 |

---

## 4. Step 2: 데이터 정리하기

### 📋 무엇을 하나요?

**"좋은 것"과 "나쁜 것"을 짝지어 비교할 수 있게 정리!**

```
원본 데이터:
┌──────────────────────────────────────────────┐
│ 평가 1: "주인공이 웃는다" → 👍               │
│ 평가 2: "남자가 있다" → 👎                   │
│ 평가 3: "여자가 걷는다" → 👍                 │
│ 평가 4: "사람이 움직인다" → 👎               │
└──────────────────────────────────────────────┘
         │
         ▼ 정리!
         
Preference Pair (선호 쌍):
┌──────────────────────────────────────────────┐
│                                              │
│  Pair 1:                                     │
│  ┌─────────────────┐  ┌─────────────────┐   │
│  │ ✅ 선호됨        │  │ ❌ 거부됨        │   │
│  │ "주인공이 웃는다"│  │ "남자가 있다"    │   │
│  │ (구체적!)       │  │ (너무 짧음)      │   │
│  └─────────────────┘  └─────────────────┘   │
│                                              │
│  Pair 2:                                     │
│  ┌─────────────────┐  ┌─────────────────┐   │
│  │ ✅ 선호됨        │  │ ❌ 거부됨        │   │
│  │ "여자가 걷는다"  │  │ "사람이 움직인다"│   │
│  │ (명확함!)       │  │ (모호함)         │   │
│  └─────────────────┘  └─────────────────┘   │
│                                              │
└──────────────────────────────────────────────┘
```

### 🎯 왜 이걸 하나요?

> **AI에게 "비교"를 보여줘야 배움!**

```
나쁜 예:
  선생님: "이건 좋은 글이야"
  학생: "왜요? 뭐가 좋은데요?"
  선생님: "...그냥 좋아"
  
좋은 예:
  선생님: "이 두 글을 비교해봐"
  선생님: "A: '남자가 있다' vs B: '정장 입은 남자가 의자에 앉아있다'"
  선생님: "B가 더 좋지? 왜냐면 더 자세하니까!"
  학생: "아! 자세하게 써야 하는구나!"
```

### 📊 편집 데이터 활용

사용자가 **직접 수정**한 경우, 최고의 학습 데이터!

```
원본 (AI가 씀):     "남자가 문을 연다"        → 👎 (암묵적)
편집 (사용자 수정): "검은 코트의 남자가       → 👍 (암묵적)
                    낡은 나무문을 조심스럽게 연다"

= 완벽한 Preference Pair!
  (같은 장면, 다른 품질)
```

### ✅ 장점 & ❌ 단점

| 장점 ✅ | 단점 ❌ |
|---------|---------|
| AI가 "비교"로 학습 가능 | 짝 맞추기가 복잡할 수 있음 |
| 편집 데이터 = 최고 품질 | 데이터가 적으면 짝이 안 맞음 |
| 학습 효과가 명확함 | 정리에 시간 소요 |

---

## 5. Step 3: AI 선생님 만들기 (Reward Model)

### 📋 무엇을 하나요?

**"좋고 나쁨을 판단하는 AI"를 먼저 만듦!**

```
┌─────────────────────────────────────────────────────┐
│                                                     │
│   🎓 Reward Model = "AI 선생님"                     │
│                                                     │
│   역할: 화면해설을 보고 점수 매기기                   │
│                                                     │
│   ┌─────────────────────────────────────────────┐  │
│   │ 입력: "정장 입은 남자가 천천히 걸어온다"       │  │
│   │                                             │  │
│   │         ↓ Reward Model ↓                    │  │
│   │                                             │  │
│   │ 출력: 점수 = 0.85 (좋음! 👍)                │  │
│   └─────────────────────────────────────────────┘  │
│                                                     │
│   ┌─────────────────────────────────────────────┐  │
│   │ 입력: "남자가 있다"                          │  │
│   │                                             │  │
│   │         ↓ Reward Model ↓                    │  │
│   │                                             │  │
│   │ 출력: 점수 = 0.23 (별로... 👎)              │  │
│   └─────────────────────────────────────────────┘  │
│                                                     │
└─────────────────────────────────────────────────────┘
```

### 🎯 왜 이걸 하나요?

> **사람이 매번 평가할 수 없으니까!**

```
문제:
  - AI 학생이 답변을 1,000,000번 연습해야 함
  - 사람이 1,000,000번 평가? 불가능! 😵

해결:
  - "AI 선생님"을 먼저 만들어서
  - AI 선생님이 대신 평가!
  
┌─────────────────────────────────────────────────┐
│  사람 평가 데이터 (1,000개)                      │
│         │                                       │
│         ▼                                       │
│  ┌─────────────────┐                            │
│  │ Reward Model    │ ← 사람 취향 학습!          │
│  │ (AI 선생님)     │                            │
│  └────────┬────────┘                            │
│           │                                     │
│           ▼                                     │
│  AI 학생 연습 1,000,000번                       │
│  → AI 선생님이 빠르게 평가! ⚡                   │
└─────────────────────────────────────────────────┘
```

### 🏗️ 어떻게 만드나요?

```python
# 쉬운 비유로 설명:

def AI선생님_학습(좋은글_목록, 나쁜글_목록):
    """
    1. 좋은 글들의 특징을 분석
    2. 나쁜 글들의 특징을 분석
    3. 차이점을 학습!
    """
    
    # 학습 후...
    
def AI선생님_평가(새로운_글):
    """
    배운 기준으로 새 글 평가
    """
    점수 = 좋은글_특징이_얼마나_있나(새로운_글)
    return 점수  # 0.0 ~ 1.0
```

### ✅ 장점 & ❌ 단점

| 장점 ✅ | 단점 ❌ |
|---------|---------|
| 빠른 평가 가능 (초당 수천 개) | 사람만큼 정확하진 않음 |
| 일관된 기준 | 학습 데이터가 편향되면 같이 편향 |
| 24시간 작동 | 만드는 데 시간/비용 필요 |

---

## 6. Step 4: AI 학생 가르치기 (Fine-tuning)

### 📋 무엇을 하나요?

**드디어 본 게임! AI를 실제로 개선시키기**

### 🎓 학습 방법 3가지

#### 방법 1: SFT (Supervised Fine-Tuning)

> **"좋은 예시만 보여주기"**

```
┌─────────────────────────────────────────────────────┐
│  SFT = 모범 답안으로 공부                            │
│                                                     │
│  선생님: "이 문제의 정답은 이거야. 외워!"             │
│                                                     │
│  학습 데이터:                                       │
│  ┌─────────────────────────────────────────────┐   │
│  │ 문제: [영상 장면 설명]                        │   │
│  │ 정답: "정장 입은 중년 남성이 회의실로 들어온다" │   │
│  └─────────────────────────────────────────────┘   │
│                                                     │
│  AI: "아, 이렇게 쓰면 되는구나!"                     │
└─────────────────────────────────────────────────────┘
```

**장단점:**
| 장점 ✅ | 단점 ❌ |
|---------|---------|
| 간단하고 빠름 | "왜" 좋은지는 모름 |
| 데이터 적어도 됨 | 다양한 상황 대응 어려움 |
| 결과 예측 가능 | 창의성 제한 |

---

#### 방법 2: DPO (Direct Preference Optimization)

> **"이거 vs 저거 중에 이게 더 좋아"를 직접 학습**

```
┌─────────────────────────────────────────────────────┐
│  DPO = 비교해서 배우기                               │
│                                                     │
│  선생님: "이 두 개 중에 왼쪽이 더 좋아. 왜인지 생각해봐"│
│                                                     │
│  학습 데이터:                                       │
│  ┌─────────────────────────────────────────────┐   │
│  │ 상황: [영상 장면]                             │   │
│  │                                             │   │
│  │ ✅ 선호: "햇살이 비치는 창가에 여자가 앉아있다"  │   │
│  │ ❌ 거부: "여자가 앉아있다"                     │   │
│  └─────────────────────────────────────────────┘   │
│                                                     │
│  AI: "아, 디테일이 중요하구나!"                      │
└─────────────────────────────────────────────────────┘
```

**장단점:**
| 장점 ✅ | 단점 ❌ |
|---------|---------|
| 선호도 직접 반영 | Preference Pair 필요 |
| Reward Model 필요 없음 | 비교 데이터 만들기 어려움 |
| 최근 인기 방법! | SFT보다는 복잡 |

---

#### 방법 3: PPO (Proximal Policy Optimization)

> **"시행착오로 배우기"** (게임하듯이!)

```
┌─────────────────────────────────────────────────────┐
│  PPO = 게임처럼 학습                                 │
│                                                     │
│  ┌─────────────────────────────────────────────┐   │
│  │                                             │   │
│  │    AI가 화면해설 작성                         │   │
│  │            │                                │   │
│  │            ▼                                │   │
│  │    Reward Model이 점수 매김                  │   │
│  │            │                                │   │
│  │            ▼                                │   │
│  │    점수 높으면 → 그 방향으로 더 학습! 📈      │   │
│  │    점수 낮으면 → 그 방향 피하기! 📉          │   │
│  │            │                                │   │
│  │            ▼                                │   │
│  │    반복... 반복... 반복...                   │   │
│  │                                             │   │
│  └─────────────────────────────────────────────┘   │
│                                                     │
│  게임에서 점수 올리려고 연습하는 것과 같음!           │
└─────────────────────────────────────────────────────┘
```

**장단점:**
| 장점 ✅ | 단점 ❌ |
|---------|---------|
| 가장 유연함 | 복잡하고 불안정 |
| 창의적 답변 가능 | Reward Model 필요 |
| ChatGPT가 쓴 방법! | 학습 오래 걸림 |

---

### 📊 3가지 방법 한눈에 비교

```
┌─────────────────────────────────────────────────────────────┐
│                    학습 방법 비교                            │
├─────────────┬─────────────┬─────────────┬─────────────────┤
│             │    SFT      │    DPO      │     PPO         │
├─────────────┼─────────────┼─────────────┼─────────────────┤
│ 난이도       │ ⭐          │ ⭐⭐        │ ⭐⭐⭐          │
│             │ 쉬움        │ 보통        │ 어려움          │
├─────────────┼─────────────┼─────────────┼─────────────────┤
│ 필요 데이터  │ 좋은 예시만  │ 비교 쌍     │ Reward Model    │
│             │             │             │ + 많은 시도     │
├─────────────┼─────────────┼─────────────┼─────────────────┤
│ 학습 시간   │ 빠름 ⚡     │ 보통        │ 느림 🐢         │
├─────────────┼─────────────┼─────────────┼─────────────────┤
│ 품질        │ 괜찮음      │ 좋음        │ 매우 좋음       │
├─────────────┼─────────────┼─────────────┼─────────────────┤
│ 추천 상황   │ 처음 시작   │ 선호도 반영  │ 최고 품질 목표  │
└─────────────┴─────────────┴─────────────┴─────────────────┘
```

### 🚀 Ko-AD 추천 순서

```
1단계: SFT 먼저!
  → 좋은 예시로 기본기 학습
  → 빠르게 어느 정도 품질 확보

2단계: DPO 적용
  → 사용자 선호도 반영
  → Preference Pair 데이터 활용

3단계 (선택): PPO
  → 최고 품질 필요할 때
  → 리소스가 충분할 때
```

---

## 7. Step 5: 시험 보기 (평가)

### 📋 무엇을 하나요?

**학습한 AI가 정말 좋아졌는지 확인!**

```
┌─────────────────────────────────────────────────────┐
│                                                     │
│   📝 시험 방법                                      │
│                                                     │
│   1. 자동 채점 (컴퓨터가 점수 매김)                   │
│      - BERTScore: 의미가 비슷한지                    │
│      - METEOR: 단어 선택이 좋은지                    │
│                                                     │
│   2. 사람 채점 (A/B 테스트)                          │
│      "옛날 AI vs 새 AI, 뭐가 더 좋아?"               │
│                                                     │
└─────────────────────────────────────────────────────┘
```

### 🔬 A/B 테스트란?

```
┌─────────────────────────────────────────────────────────┐
│                                                         │
│   사용자 100명                                          │
│       │                                                 │
│       ├──► 50명: 옛날 AI (A그룹)                        │
│       │         "남자가 걷는다"                         │
│       │                                                 │
│       └──► 50명: 새 AI (B그룹)                          │
│                 "정장 입은 남자가 복도를 걸어간다"        │
│                                                         │
│   결과 비교:                                            │
│   ┌─────────────────────────────────────────────┐      │
│   │ A그룹 👍: 30%  │  B그룹 👍: 75%            │      │
│   │                                             │      │
│   │ → B(새 AI)가 훨씬 좋음! 성공! 🎉            │      │
│   └─────────────────────────────────────────────┘      │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### 📊 측정하는 것들

| 지표 | 의미 | 좋은 값 |
|------|------|---------|
| 👍 비율 | 좋아요 누른 비율 | 높을수록 좋음 |
| 👎 비율 | 싫어요 누른 비율 | 낮을수록 좋음 |
| ✏️ 수정 비율 | 사용자가 수정한 비율 | 낮을수록 좋음 |
| ⏱️ 승인 시간 | 확인하는 데 걸린 시간 | 짧을수록 좋음 |

---

## 8. 학습 방법 비교

### 🎯 목적별 추천

```
┌──────────────────────────────────────────────────────────┐
│                                                          │
│  "빨리 시작하고 싶어요"                                   │
│  → SFT (Supervised Fine-Tuning)                         │
│  → 좋은 예시 100개만 있으면 시작 가능!                    │
│                                                          │
├──────────────────────────────────────────────────────────┤
│                                                          │
│  "사용자 취향을 반영하고 싶어요"                           │
│  → DPO (Direct Preference Optimization)                 │
│  → 👍/👎 평가 데이터로 학습!                             │
│                                                          │
├──────────────────────────────────────────────────────────┤
│                                                          │
│  "최고의 품질을 원해요"                                   │
│  → PPO (Proximal Policy Optimization)                   │
│  → 시간과 리소스가 많이 필요하지만 최고 품질!              │
│                                                          │
├──────────────────────────────────────────────────────────┤
│                                                          │
│  "돈 주고 쉽게 하고 싶어요"                               │
│  → OpenAI Fine-tuning                                   │
│  → API로 간편하게! (비용 발생)                            │
│                                                          │
└──────────────────────────────────────────────────────────┘
```

### 💰 비용 비교

| 방법 | 컴퓨팅 비용 | 데이터 비용 | 시간 | 난이도 |
|------|-------------|-------------|------|--------|
| SFT | 💵 | 💵 | ⏱️ | 😊 |
| DPO | 💵💵 | 💵💵 | ⏱️⏱️ | 😐 |
| PPO | 💵💵💵 | 💵💵💵 | ⏱️⏱️⏱️ | 😰 |
| OpenAI | 💵💵 (API) | 💵 | ⏱️ | 😊 |

---

## 9. 자주 묻는 질문

### ❓ Q1: 데이터가 얼마나 필요해요?

```
┌─────────────────────────────────────────────────────┐
│                                                     │
│  📊 최소 권장량                                      │
│                                                     │
│  • SFT: 100 ~ 500개                                │
│  • DPO: 500 ~ 1,000개 (Preference Pair)            │
│  • PPO: 1,000개 이상 (+ Reward Model용 데이터)      │
│                                                     │
│  💡 팁: 적은 데이터로 시작해서 점점 늘려가기!         │
│                                                     │
└─────────────────────────────────────────────────────┘
```

### ❓ Q2: 학습 시간은 얼마나 걸려요?

```
GPU 1개 (RTX 3090 기준):

• SFT: 1~2시간
• DPO: 3~6시간  
• PPO: 12시간 ~ 며칠

💡 클라우드 GPU 사용하면 더 빠름!
   (AWS, Google Cloud, etc.)
```

### ❓ Q3: 실패하면 어떻게 해요?

```
흔한 실패 원인 & 해결책:

1. "성능이 안 좋아졌어요"
   → 데이터 품질 확인 (노이즈 제거)
   → 학습률(learning rate) 낮추기
   
2. "학습이 안 돼요"
   → 데이터 형식 확인
   → 메모리 부족이면 배치 사이즈 줄이기
   
3. "결과가 이상해요"
   → 평가 데이터 균형 확인 (👍 vs 👎)
   → 오버피팅 의심 → 학습 에폭 줄이기
```

### ❓ Q4: OpenAI vs 직접 학습, 뭐가 좋아요?

```
┌─────────────────────────────────────────────────────┐
│                                                     │
│  OpenAI Fine-tuning 추천:                           │
│  • 빨리 시작하고 싶을 때                             │
│  • GPU가 없을 때                                    │
│  • 간편함이 중요할 때                               │
│                                                     │
│  직접 학습 (Hugging Face 등) 추천:                   │
│  • 비용을 아끼고 싶을 때                             │
│  • 모델을 완전히 소유하고 싶을 때                    │
│  • 커스텀이 많이 필요할 때                           │
│                                                     │
└─────────────────────────────────────────────────────┘
```

---

## 🎯 요약: 핵심만 기억하기!

```
┌─────────────────────────────────────────────────────────────┐
│                                                             │
│   🎓 RLHF = "사람 피드백으로 AI 가르치기"                    │
│                                                             │
│   📝 5단계 과정:                                            │
│   1. 👍👎 평가 모으기                                       │
│   2. "좋은 것 vs 나쁜 것" 비교 데이터 만들기                 │
│   3. AI 선생님 (Reward Model) 만들기                        │
│   4. AI 학생 가르치기 (SFT → DPO → PPO)                     │
│   5. 잘 배웠는지 시험 (A/B 테스트)                          │
│                                                             │
│   🚀 시작 방법:                                             │
│   • 쉬운 것부터: SFT로 시작!                                │
│   • 데이터 모이면: DPO 적용!                                │
│   • 최고 품질 원하면: PPO 도전!                              │
│                                                             │
│   💡 핵심:                                                  │
│   "좋은 데이터 = 좋은 AI"                                   │
│   평가 데이터를 꾸준히 모으는 게 가장 중요! 📊               │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 📚 더 배우고 싶다면?

| 주제 | 자료 |
|------|------|
| RLHF 기초 | [Hugging Face RLHF 블로그](https://huggingface.co/blog/rlhf) |
| DPO 논문 | "Direct Preference Optimization" (2023) |
| PPO 논문 | "Proximal Policy Optimization" (2017) |
| 실습 코드 | [TRL 라이브러리](https://github.com/huggingface/trl) |

---

**질문 있으면 편하게 물어보세요!** 😊

